{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5386de6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48f7878b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4eb888db",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:\\\\others_projects\\\\suma\\\\text_input.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mD:\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mothers_projects\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43msuma\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mtext_input.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m      2\u001b[0m     data \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    284\u001b[0m     )\n\u001b[1;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:\\\\others_projects\\\\suma\\\\text_input.txt'"
     ]
    }
   ],
   "source": [
    "with open('text_input.txt', encoding='utf-8') as file:\n",
    "    data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01241f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup=BeautifulSoup(data,'html.parser')#remove html tag using beautifulsoup\n",
    "non_html_text=soup.get_text()\n",
    "    \n",
    "#remove unwanted charectors and symbols\n",
    "text=re.sub('[^a-zA-Z0-9\\s]',' ',non_html_text)\n",
    "text = text.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '')\n",
    "\n",
    "#remove extra spaces\n",
    "z = []\n",
    "for i in text.split():\n",
    "    if i not in z:\n",
    "        z.append(i)  \n",
    "text = ' '.join(z)\n",
    "\n",
    "#tokenize text\n",
    "    \n",
    "tokenizer=Tokenizer()\n",
    "tokenizer.fit_on_texts([text])\n",
    "sequence=tokenizer.texts_to_sequences([text])[0]\n",
    "\n",
    "sequence[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecca4060",
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_dim=len(tokenizer.word_index)+1\n",
    "ip_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e090df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM,Dense,Embedding\n",
    "model=Sequential()\n",
    "model.add(Embedding(ip_dim,10,input_length=3))\n",
    "model.add(LSTM(1000,return_sequences=True))\n",
    "model.add(LSTM(1000))\n",
    "model.add(Dense(1000, activation=\"relu\"))\n",
    "model.add(Dense(ip_dim,activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ea3ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1f452e-8cc7-40fc-8c81-fbd742e2390a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('server.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8937fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def federated_averaging(global_weights, client_weights_list):\n",
    "    num_clients = len(client_weights_list)\n",
    "    averaged_weights = global_weights.copy()\n",
    "    \n",
    "    for i in range(len(averaged_weights)):\n",
    "        # Initialize with the global weights\n",
    "        averaged_weights[i] = global_weights[i]\n",
    "        \n",
    "        # Aggregate model updates from all clients\n",
    "        for client_weights in client_weights_list:\n",
    "            averaged_weights[i] += client_weights[i] / num_clients\n",
    "    \n",
    "    return averaged_weights\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9ca022",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Define the server's IP address and port\n",
    "server_ip = '192.168.94.137'  # Replace with your server's local IP address\n",
    "server_port = 12347\n",
    "\n",
    "# Create a server socket\n",
    "server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "\n",
    "# Bind the server socket to the IP address and port\n",
    "server_socket.bind((server_ip, server_port))\n",
    "\n",
    "# Listen for incoming connections\n",
    "server_socket.listen(3)\n",
    "\n",
    "print(f\"Server is listening for incoming connections on {server_ip}:{server_port}...\")\n",
    "\n",
    "\n",
    "# Accept connections from multiple clients\n",
    "# Simulate federated learning rounds\n",
    "num_rounds = 6\n",
    "num_clients= 3\n",
    "    \n",
    "# Receive model weights from each client\n",
    "client_weights_list = []\n",
    "\n",
    "for _ in range(num_clients):\n",
    "    client_socket, client_address = server_socket.accept()\n",
    "    print(f\"Accepted connection from {client_address}\")\n",
    "    \n",
    "    client_weights_bytes = b''\n",
    "    for round_num in range(num_rounds):\n",
    "        while True:\n",
    "            data = client_socket.recv(13246848)  # Adjust buffer size as needed\n",
    "            if not data:\n",
    "                break\n",
    "            client_weights_bytes += data\n",
    "            \n",
    "\n",
    "        if not client_weights_bytes:\n",
    "            print(f\"No data received from a client{_ + 1}, in round {round_num}.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            client_model_weights = pickle.loads(client_weights_bytes)\n",
    "            client_weights_list.append(client_model_weights)\n",
    "            \n",
    "        except pickle.UnpicklingError as e:\n",
    "            print(f\"Error while unpickling data from a client: {e}\")\n",
    "            \n",
    "    client_socket.close()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d2ad92",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Aggregate model updates using federated averaging\n",
    "if client_weights_list:\n",
    "    global_weights = federated_averaging(model.get_weights(), client_weights_list)\n",
    "    model.set_weights(global_weights)\n",
    "    print(f\"Updated global model with data from {num_clients} clients in round {len(client_weights_list)}.\")\n",
    "\n",
    "        \n",
    "# The global model now contains the federated learning result\n",
    "model.save_weights('model_weights.h5')\n",
    "\n",
    "# Close the server socket\n",
    "server_socket.close()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d79c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# For preprocessing input text for test\n",
    "def preprocess(text):\n",
    "    soup1=BeautifulSoup(text,'html.parser')#remove html tag using beautifulsoup\n",
    "    non_html_txt=soup.get_text()\n",
    "    \n",
    "    #remove unwanted charectors and symbols\n",
    "    text=re.sub('[^a-zA-Z0-9\\s]',' ',non_html_txt)\n",
    "    text = text.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '')\n",
    "\n",
    "    #remove extra spaces\n",
    "    z = []\n",
    "    for i in text.split():\n",
    "        if i not in z:\n",
    "            z.append(i)  \n",
    "    text = ' '.join(z)\n",
    "    return text\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0432c515",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# It gives single predicted words\n",
    "def predict_nxt_word(txt):\n",
    "\n",
    "    tok_text = tokenizer.texts_to_sequences([ txt])\n",
    "    #print(tok_text)\n",
    "    preds = model.predict(np.array(tok_text), verbose=0)[0]\n",
    "    next_word = tokenizer.sequences_to_texts([[np.argmax(preds)]])[0]\n",
    "    return next_word\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45fe9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "txt='Can I'\n",
    "txt=preprocess(txt)\n",
    "txt=txt.split(' ')\n",
    "txt=txt[-3:]\n",
    "# print(txt)\n",
    "nxt=predict_nxt_word(txt)\n",
    "print(nxt)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2aa013",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#formula caluclation\n",
    "data_size = 79481088  # Bytes\n",
    "num_iterations = 6\n",
    "\n",
    "buffer_size = data_size // num_iterations\n",
    "\n",
    "print(buffer_size)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eda761c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
